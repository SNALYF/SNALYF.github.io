[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am currently a master’s student at the University of British Columbia (UBC), majoring in Data Science, Computational Linguistics.\nMy academic journey is driven by a fascination with patterns—whether they are hidden in large datasets, embedded in human language, or structured within musical compositions."
  },
  {
    "objectID": "about.html#my-journey",
    "href": "about.html#my-journey",
    "title": "About Me",
    "section": "",
    "text": "I am currently a master’s student at the University of British Columbia (UBC), majoring in Data Science, Computational Linguistics.\nMy academic journey is driven by a fascination with patterns—whether they are hidden in large datasets, embedded in human language, or structured within musical compositions."
  },
  {
    "objectID": "about.html#technical-philosophy",
    "href": "about.html#technical-philosophy",
    "title": "About Me",
    "section": "Technical Philosophy",
    "text": "Technical Philosophy\nI don’t just “run code.” I believe in understanding the mathematical foundations behind the algorithms.\n\nIn Data Science: My background in Statistics and Computer Science allows me to deeply understand model behaviors, rather than being only a user of black-box models.\nIn Economics: My background in microeconomics and macroeconomics allows me to combine Economic theories with Data Science."
  },
  {
    "objectID": "about.html#experience-mentorship",
    "href": "about.html#experience-mentorship",
    "title": "About Me",
    "section": "Experience & Mentorship",
    "text": "Experience & Mentorship\nMentoring Program | Mentee Working with Jacob Oh (Jan 2026 - Present) Participating in a professional mentorship program to bridge the gap between academic theory and industry application. Focusing on career development in tech and software engineering best practices."
  },
  {
    "objectID": "about.html#beyond-the-code",
    "href": "about.html#beyond-the-code",
    "title": "About Me",
    "section": "Beyond the Code",
    "text": "Beyond the Code\nWhen I’m not coding in Python or analyzing data, I am likely exploring Music Theory. I am fascinated by how chords and scales function mathematically. This hobby actually complements my coding work, as both require recognizing structures and creative problem-solving within a set of rules."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Credit Card Default Analysis\n\n\n\nPython\n\nSVC\n\nLinear Regression\n\nRandom Forest\n\nHistGradientBoosting\n\nXGBoosting\n\n\n\nUsing XGBoosting to predict credit card defaults\n\n\n\nTianhao Cao\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeather Insurance Purchasing Prediction\n\n\n\nLasso\n\nRandom Forest\n\nCross-Validation\n\nEconometrics\n\n\n\nPredicting farmers’ insurance buying behavior using Lasso and Random Forest\n\n\n\nTianhao Cao\n\n\nJun 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid-19 DiD Analysis\n\n\n\nDifference-in-Difference\n\nLinear Regression\n\n\n\nIdentifying the influences of Early-Policies to the COVID-19 transmission and Deaths\n\n\n\nTianhao Cao\n\n\nMar 18, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/credit card default analysis.html",
    "href": "posts/credit card default analysis.html",
    "title": "Credit Card Default Analysis",
    "section": "",
    "text": "A project of credit card default analysis and corresponding prediction.\nhttps://github.com/SNALYF/Credit-Card-Default-Analysis\n\n\n\nEDA: Cleaned and implemented EDA to dataset, navigated on the correlations between numerical features and categorical features\nModel Selection: Conducted logistic model (Test F1-score: 0.530), SVC model (Test F1-score: 0.532), HistGradientBoosting (0.534), and XGBoosting model(0.507).\nHyperparameter Optimization: Implemented Randomized Search Cross-Validation on SVC, HistGradientBoosting, and XGBoosting model to obtain the highest F1-score.\nShap Interpretation: Analyzed SHAP plot to interpret black-box model decisions, identified most important feature.\n\n\n\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import randint, loguniform, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nimport joblib\n\nfrom .config import MODELS_DIR, RESULTS_DIR, RANDOM_STATE\nfrom .load_data import load_raw_data, train_test_split_data\nfrom .features import engineer_features\nfrom .preprocess import build_tree_preprocessor\nfrom .utils import ensure_dir, save_or_update_json\n\n\ndef build_xgb_pipeline(X_train_eng: pd.DataFrame, y_train) -&gt; RandomizedSearchCV:\n    \"\"\"\n    Build an XGBoost + preprocessing pipeline and wrap it in RandomizedSearchCV\n    to optimize multiple hyperparameters (same idea as in the notebook).\n    \"\"\"\n    # Preprocessor\n    preprocessor = build_tree_preprocessor(X_train_eng)\n\n    # Class imbalance weight\n    neg = np.sum(y_train == 0)\n    pos = np.sum(y_train == 1)\n    scale_pos_weight = neg / pos\n\n    xgb = XGBClassifier(\n        random_state=RANDOM_STATE,\n        scale_pos_weight=scale_pos_weight,\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n    )\n\n    pipe = Pipeline(\n        steps=[\n            (\"preprocessor\", preprocessor),\n            (\"xgbclassifier\", xgb),\n        ]\n    )\n\n    # Hyperparameter search space (mirrors notebook)\n    param_distributions = {\n        \"xgbclassifier__n_estimators\": randint(100, 1000),\n        \"xgbclassifier__learning_rate\": loguniform(0.001, 0.3),\n        \"xgbclassifier__max_depth\": randint(3, 10),\n        \"xgbclassifier__subsample\": uniform(0.6, 0.4),\n        \"xgbclassifier__colsample_bytree\": uniform(0.6, 0.4),\n        \"xgbclassifier__min_child_weight\": randint(1, 10),\n        \"xgbclassifier__gamma\": uniform(0.0, 5.0),\n        \"xgbclassifier__reg_lambda\": loguniform(1e-3, 10.0),\n        \"xgbclassifier__reg_alpha\": loguniform(1e-3, 10.0),\n    }\n\n    search = RandomizedSearchCV(\n        pipe,\n        param_distributions=param_distributions,\n        n_jobs=-1,\n        scoring=\"f1\",\n        return_train_score=True,\n        random_state=RANDOM_STATE,\n        verbose=1,\n    )\n\n    return search\n\n\ndef main() -&gt; None:\n\n    df = load_raw_data()\n    X_train, X_test, y_train, y_test = train_test_split_data(df)\n\n    # ---------- Feature engineering ----------\n    X_train_eng = engineer_features(X_train)\n\n    # ---------- Build & tune model ----------\n    search = build_xgb_pipeline(X_train_eng, y_train)\n    search.fit(X_train_eng, y_train)\n\n    best_estimator = search.best_estimator_\n    best_row = (\n        pd.DataFrame(search.cv_results_)[\n            [\"mean_test_score\", \"mean_train_score\"]\n        ]\n        .sort_values(\"mean_test_score\", ascending=False)\n        .iloc[0]\n    )\n\n    # ---------- Save model ----------\n    ensure_dir(MODELS_DIR)\n    model_path = MODELS_DIR / \"xgb_best_model.pkl\"\n    joblib.dump(best_estimator, model_path)\n    print(f\"Saved best model to {model_path}\")\n\n    # ---------- Save validation metrics ----------\n    ensure_dir(RESULTS_DIR)\n    metrics_path = RESULTS_DIR / \"metrics.json\"\n    val_metrics = {\n        \"validation\": {\n            \"mean_f1_val\": float(best_row[\"mean_test_score\"]),\n            \"mean_f1_train\": float(best_row[\"mean_train_score\"]),\n            \"best_params\": search.best_params_,\n        }\n    }\n    save_or_update_json(val_metrics, metrics_path)\n    print(f\"Validation metrics saved to {metrics_path}\")"
  },
  {
    "objectID": "posts/credit card default analysis.html#project-overview",
    "href": "posts/credit card default analysis.html#project-overview",
    "title": "Credit Card Default Analysis",
    "section": "",
    "text": "A project of credit card default analysis and corresponding prediction.\nhttps://github.com/SNALYF/Credit-Card-Default-Analysis\n\n\n\nEDA: Cleaned and implemented EDA to dataset, navigated on the correlations between numerical features and categorical features\nModel Selection: Conducted logistic model (Test F1-score: 0.530), SVC model (Test F1-score: 0.532), HistGradientBoosting (0.534), and XGBoosting model(0.507).\nHyperparameter Optimization: Implemented Randomized Search Cross-Validation on SVC, HistGradientBoosting, and XGBoosting model to obtain the highest F1-score.\nShap Interpretation: Analyzed SHAP plot to interpret black-box model decisions, identified most important feature.\n\n\n\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import randint, loguniform, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nimport joblib\n\nfrom .config import MODELS_DIR, RESULTS_DIR, RANDOM_STATE\nfrom .load_data import load_raw_data, train_test_split_data\nfrom .features import engineer_features\nfrom .preprocess import build_tree_preprocessor\nfrom .utils import ensure_dir, save_or_update_json\n\n\ndef build_xgb_pipeline(X_train_eng: pd.DataFrame, y_train) -&gt; RandomizedSearchCV:\n    \"\"\"\n    Build an XGBoost + preprocessing pipeline and wrap it in RandomizedSearchCV\n    to optimize multiple hyperparameters (same idea as in the notebook).\n    \"\"\"\n    # Preprocessor\n    preprocessor = build_tree_preprocessor(X_train_eng)\n\n    # Class imbalance weight\n    neg = np.sum(y_train == 0)\n    pos = np.sum(y_train == 1)\n    scale_pos_weight = neg / pos\n\n    xgb = XGBClassifier(\n        random_state=RANDOM_STATE,\n        scale_pos_weight=scale_pos_weight,\n        use_label_encoder=False,\n        eval_metric=\"logloss\",\n    )\n\n    pipe = Pipeline(\n        steps=[\n            (\"preprocessor\", preprocessor),\n            (\"xgbclassifier\", xgb),\n        ]\n    )\n\n    # Hyperparameter search space (mirrors notebook)\n    param_distributions = {\n        \"xgbclassifier__n_estimators\": randint(100, 1000),\n        \"xgbclassifier__learning_rate\": loguniform(0.001, 0.3),\n        \"xgbclassifier__max_depth\": randint(3, 10),\n        \"xgbclassifier__subsample\": uniform(0.6, 0.4),\n        \"xgbclassifier__colsample_bytree\": uniform(0.6, 0.4),\n        \"xgbclassifier__min_child_weight\": randint(1, 10),\n        \"xgbclassifier__gamma\": uniform(0.0, 5.0),\n        \"xgbclassifier__reg_lambda\": loguniform(1e-3, 10.0),\n        \"xgbclassifier__reg_alpha\": loguniform(1e-3, 10.0),\n    }\n\n    search = RandomizedSearchCV(\n        pipe,\n        param_distributions=param_distributions,\n        n_jobs=-1,\n        scoring=\"f1\",\n        return_train_score=True,\n        random_state=RANDOM_STATE,\n        verbose=1,\n    )\n\n    return search\n\n\ndef main() -&gt; None:\n\n    df = load_raw_data()\n    X_train, X_test, y_train, y_test = train_test_split_data(df)\n\n    # ---------- Feature engineering ----------\n    X_train_eng = engineer_features(X_train)\n\n    # ---------- Build & tune model ----------\n    search = build_xgb_pipeline(X_train_eng, y_train)\n    search.fit(X_train_eng, y_train)\n\n    best_estimator = search.best_estimator_\n    best_row = (\n        pd.DataFrame(search.cv_results_)[\n            [\"mean_test_score\", \"mean_train_score\"]\n        ]\n        .sort_values(\"mean_test_score\", ascending=False)\n        .iloc[0]\n    )\n\n    # ---------- Save model ----------\n    ensure_dir(MODELS_DIR)\n    model_path = MODELS_DIR / \"xgb_best_model.pkl\"\n    joblib.dump(best_estimator, model_path)\n    print(f\"Saved best model to {model_path}\")\n\n    # ---------- Save validation metrics ----------\n    ensure_dir(RESULTS_DIR)\n    metrics_path = RESULTS_DIR / \"metrics.json\"\n    val_metrics = {\n        \"validation\": {\n            \"mean_f1_val\": float(best_row[\"mean_test_score\"]),\n            \"mean_f1_train\": float(best_row[\"mean_train_score\"]),\n            \"best_params\": search.best_params_,\n        }\n    }\n    save_or_update_json(val_metrics, metrics_path)\n    print(f\"Validation metrics saved to {metrics_path}\")"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF Version"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nUniversity of British Columbia (UBC) | Vancouver, BC\nM.S. in Data Science, Computational Linguistics | Aug 2025 - Present\n\nRelevant Coursework: Supervised Learning, Unsupervised Learning, Natural Language Processing, Machine Learning, Statistics.\nAcademic Focus: Examine the patterns in data and build models to predict the future.\n\nUniversity of California, Santa Cruz(UCSC) | Santa Cruz, CA\nB.A. in Business Management Economics | Sep 2019 - June 2022\n\nRelevant Coursework: Advanced Quantitative Analysis, Machine Learning Economics, Statistics, Security Market.\nAcademic Focus: Digging into the Economical patterns by using Machine Learning and Statistics."
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\nCategory\nSkills\n\n\n\n\nLanguages\nPython (Advanced), R, SQL, Bash\n\n\nData Science\nPandas, NumPy, Scikit-Learn, PyTorch\n\n\nNLP\nWord Embeddings, N-gram Models, Parsing Algorithms\n\n\nTools\nGit/GitHub, VS Code, Jupyter, Quarto, LaTeX\n\n\nPlatforms\nmacOS, Linux/Unix Environment"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\nCareer Mentorship Program | Mentee Jan 2026 - Present\n\nSelected for a professional mentorship program under industry veteran Jacob Oh.\nFocusing on software engineering best practices, career development in tech, and bridging academic concepts to industry standards."
  },
  {
    "objectID": "resume.html#capstone",
    "href": "resume.html#capstone",
    "title": "Resume",
    "section": "Capstone",
    "text": "Capstone\nTBD"
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html",
    "href": "posts/weather insurance purchasing prediction.html",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "",
    "text": "This project focuses on quantifying factors influencing Chinese farmers’ decisions to purchase weather insurance and predicting their purchasing probabilities. Using a dataset from Jiangxi, China, which includes 4,902 observations and 59 variables[cite: 30], the study navigates the challenges of predicting human economic behavior in a low-dimensional setting.\nBy implementing Lasso Cross-Validation and Random Forest algorithms, the project contrasts linear regularization methods with non-linear tree-based methods to identify significant determinants such as “Understanding” and “Social Network”.\n\n\n\nVariable Selection with Lasso: Utilized Lasso regularization to handle the bias-variance trade-off. The optimal \\(\\lambda\\) zeroed out 14 regressors, highlighting “Understanding” and “Network-related” variables as the most significant predictors.\nEnsemble Learning (Random Forest): Constructed a Random Forest model with 300 trees to capture non-linear relationships. Results revealed that unlike in the Lasso model, “Age” played a considerable role in purchasing decisions, while “Risk Averse” traits were surprisingly less important.\nPerformance Evaluation: Evaluated models using ROC Curves, Sensitivity/Specificity trade-offs, and \\(R^2\\). While the Lasso model achieved ~64% prediction correctness, the low \\(R^2\\) (&lt; 0.1) across both models highlighted the complexity of behavioral prediction and potential unobserved variables.\nData Cleaning & Bias Analysis: Processed raw data by handling NA values and removing variables with &gt;1000 missing entries. Conducted a critical analysis of data limitations, acknowledging potential biases from omitted variables like modern technology adoption."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#project-overview",
    "href": "posts/weather insurance purchasing prediction.html#project-overview",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "",
    "text": "This project focuses on quantifying factors influencing Chinese farmers’ decisions to purchase weather insurance and predicting their purchasing probabilities. Using a dataset from Jiangxi, China, which includes 4,902 observations and 59 variables[cite: 30], the study navigates the challenges of predicting human economic behavior in a low-dimensional setting.\nBy implementing Lasso Cross-Validation and Random Forest algorithms, the project contrasts linear regularization methods with non-linear tree-based methods to identify significant determinants such as “Understanding” and “Social Network”.\n\n\n\nVariable Selection with Lasso: Utilized Lasso regularization to handle the bias-variance trade-off. The optimal \\(\\lambda\\) zeroed out 14 regressors, highlighting “Understanding” and “Network-related” variables as the most significant predictors.\nEnsemble Learning (Random Forest): Constructed a Random Forest model with 300 trees to capture non-linear relationships. Results revealed that unlike in the Lasso model, “Age” played a considerable role in purchasing decisions, while “Risk Averse” traits were surprisingly less important.\nPerformance Evaluation: Evaluated models using ROC Curves, Sensitivity/Specificity trade-offs, and \\(R^2\\). While the Lasso model achieved ~64% prediction correctness, the low \\(R^2\\) (&lt; 0.1) across both models highlighted the complexity of behavioral prediction and potential unobserved variables.\nData Cleaning & Bias Analysis: Processed raw data by handling NA values and removing variables with &gt;1000 missing entries. Conducted a critical analysis of data limitations, acknowledging potential biases from omitted variables like modern technology adoption."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#paper-preview",
    "href": "posts/weather insurance purchasing prediction.html#paper-preview",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#link-to-repo",
    "href": "posts/weather insurance purchasing prediction.html#link-to-repo",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "posts/covid19 did analysis.html",
    "href": "posts/covid19 did analysis.html",
    "title": "Covid-19 DiD Analysis",
    "section": "",
    "text": "This project estimates the early policy effects on COVID-19 transmission and damages through four dependent variables: daily new cases, cumulative cases, daily deaths, and cumulative deaths. The data is provided by the World Health Organization and Our World in Data.\nTwo different Difference-in-Difference models were implemented to estimate the effects of Mandatory policies, Optional policies, and no-control policies. The analysis found that mandatory policies had the most significant impact on limiting and decreasing COVID-19 transmission and damages, followed by optional policies, while no-control policies were the least effective.\n\n\n\nDifference-in-Difference (DiD): Utilized DiD models to compare the impact of different policy stringency levels on infection and death rates over time.\nPolicy Categorization: Countries were categorized into three groups based on early policy stringency:\n\nMandatory Countries (e.g., China): Strict travel restrictions and enforced preventive measures.\nOptional Countries (e.g., Canada, Japan, UK): Advised but not strictly enforced measures.\nNo-control Countries (e.g., USA, India): Limited restrictions and guidance.\n\nStatistical Analysis: Analyzed the effects on new cases, cumulative cases, new deaths, and cumulative deaths using empiricial statistical methods."
  },
  {
    "objectID": "posts/covid19 did analysis.html#project-overview",
    "href": "posts/covid19 did analysis.html#project-overview",
    "title": "Covid-19 DiD Analysis",
    "section": "",
    "text": "This project estimates the early policy effects on COVID-19 transmission and damages through four dependent variables: daily new cases, cumulative cases, daily deaths, and cumulative deaths. The data is provided by the World Health Organization and Our World in Data.\nTwo different Difference-in-Difference models were implemented to estimate the effects of Mandatory policies, Optional policies, and no-control policies. The analysis found that mandatory policies had the most significant impact on limiting and decreasing COVID-19 transmission and damages, followed by optional policies, while no-control policies were the least effective.\n\n\n\nDifference-in-Difference (DiD): Utilized DiD models to compare the impact of different policy stringency levels on infection and death rates over time.\nPolicy Categorization: Countries were categorized into three groups based on early policy stringency:\n\nMandatory Countries (e.g., China): Strict travel restrictions and enforced preventive measures.\nOptional Countries (e.g., Canada, Japan, UK): Advised but not strictly enforced measures.\nNo-control Countries (e.g., USA, India): Limited restrictions and guidance.\n\nStatistical Analysis: Analyzed the effects on new cases, cumulative cases, new deaths, and cumulative deaths using empiricial statistical methods."
  },
  {
    "objectID": "posts/covid19 did analysis.html#paper-preview",
    "href": "posts/covid19 did analysis.html#paper-preview",
    "title": "Covid-19 DiD Analysis",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/covid19 did analysis.html#link-to-repo",
    "href": "posts/covid19 did analysis.html#link-to-repo",
    "title": "Covid-19 DiD Analysis",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tianhao Cao",
    "section": "",
    "text": "I am a student at the University of British Columbia (UBC), passionate about bridging the gap between Statistics and Computer Science.\nMy primary interests lie in Data Science, Machine Learning, and Natural Language Processing (NLP). I enjoy building tools that solve real-world problems—whether it’s digging underlying logics behind phenomena or constructing fundamental model/algorithm to predict the output.\n\nEducation\nUniversity of British Columbia | Vancouver, BC, Canada\n\nM.S. in Data Science, Computational Linguistics | August 2025 - Present\n\nUniversity of California, Santa Cruz | Santa Cruz, CA, United States\n\nB.A. in Business Management Economics | Sept 2019 - June 2022\n\n\n\nInterests\n\nProgramming: Python (Pandas, NumPy, ScikitLearn, PyTorch), R, SQL\nData Science: Machine Learning, Data Analaysis, NLP (Word Embeddings, Parsing)\nOther: Music Theory, Mathematics"
  }
]