[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am currently a master’s student at the University of British Columbia (UBC), majoring in Data Science, Computational Linguistics.\nMy academic journey is driven by a fascination with patterns—whether they are hidden in large datasets, embedded in human language, or structured within musical compositions."
  },
  {
    "objectID": "about.html#my-journey",
    "href": "about.html#my-journey",
    "title": "About Me",
    "section": "",
    "text": "I am currently a master’s student at the University of British Columbia (UBC), majoring in Data Science, Computational Linguistics.\nMy academic journey is driven by a fascination with patterns—whether they are hidden in large datasets, embedded in human language, or structured within musical compositions."
  },
  {
    "objectID": "about.html#technical-philosophy",
    "href": "about.html#technical-philosophy",
    "title": "About Me",
    "section": "Technical Philosophy",
    "text": "Technical Philosophy\nI don’t just “run code.” I believe in understanding the mathematical foundations behind the algorithms.\n\nIn Data Science: My background in Statistics and Computer Science allows me to deeply understand model behaviors, rather than being only a user of black-box models.\nIn Economics: My background in microeconomics and macroeconomics allows me to combine Economic theories with Data Science."
  },
  {
    "objectID": "about.html#experience-mentorship",
    "href": "about.html#experience-mentorship",
    "title": "About Me",
    "section": "Experience & Mentorship",
    "text": "Experience & Mentorship\nMentoring Program | Mentee Working with Jacob Oh (Jan 2026 - Present) Participating in a professional mentorship program to bridge the gap between academic theory and industry application. Focusing on career development in tech and software engineering best practices."
  },
  {
    "objectID": "about.html#beyond-the-code",
    "href": "about.html#beyond-the-code",
    "title": "About Me",
    "section": "Beyond the Code",
    "text": "Beyond the Code\nWhen I’m not coding in Python or analyzing data, I am likely exploring Music Theory. I am fascinated by how chords and scales function mathematically. This hobby actually complements my coding work, as both require recognizing structures and creative problem-solving within a set of rules."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Sentiment Analysis on Yelp Reviews\n\n\n\nNatural Language Processing\n\nPytorch\n\nLSTM\n\nDeep Learning\n\n\n\nComparative analysis of CBOW and LSTM models for sentiment classification on Yelp reviews\n\n\n\nTianhao Cao\n\n\nFeb 1, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit Card Default Analysis\n\n\n\nPython\n\nSVC\n\nLogistic Regression\n\nRandom Forest\n\nHistGradientBoosting\n\nXGBoosting\n\n\n\nUsing XGBoosting to predict credit card defaults\n\n\n\nTianhao Cao\n\n\nDec 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeather Insurance Purchasing Prediction\n\n\n\nLasso\n\nRandom Forest\n\nCross-Validation\n\nEconometrics\n\n\n\nPredicting farmers’ insurance buying behavior using Lasso and Random Forest\n\n\n\nTianhao Cao\n\n\nJun 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCovid-19 DiD Analysis\n\n\n\nDifference-in-Difference\n\nLinear Regression\n\n\n\nIdentifying the influences of Early-Policies to the COVID-19 transmission and Deaths\n\n\n\nTianhao Cao\n\n\nMar 18, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/credit card default analysis.html",
    "href": "posts/credit card default analysis.html",
    "title": "Credit Card Default Analysis",
    "section": "",
    "text": "A project of credit card default analysis and corresponding prediction.\nhttps://github.com/SNALYF/Credit-Card-Default-Analysis\n\n\n\nEDA: Cleaned and implemented EDA to dataset, navigated on the correlations between numerical features and categorical features\nModel Selection: Conducted logistic model (Test F1-score: 0.530), SVC model (Test F1-score: 0.532), HistGradientBoosting (0.534), and XGBoosting model(0.507).\nHyperparameter Optimization: Implemented Randomized Search Cross-Validation on SVC, HistGradientBoosting, and XGBoosting model to obtain the highest F1-score.\nShap Interpretation: Analyzed SHAP plot to interpret black-box model decisions, identified most important feature."
  },
  {
    "objectID": "posts/credit card default analysis.html#project-overview",
    "href": "posts/credit card default analysis.html#project-overview",
    "title": "Credit Card Default Analysis",
    "section": "",
    "text": "A project of credit card default analysis and corresponding prediction.\nhttps://github.com/SNALYF/Credit-Card-Default-Analysis\n\n\n\nEDA: Cleaned and implemented EDA to dataset, navigated on the correlations between numerical features and categorical features\nModel Selection: Conducted logistic model (Test F1-score: 0.530), SVC model (Test F1-score: 0.532), HistGradientBoosting (0.534), and XGBoosting model(0.507).\nHyperparameter Optimization: Implemented Randomized Search Cross-Validation on SVC, HistGradientBoosting, and XGBoosting model to obtain the highest F1-score.\nShap Interpretation: Analyzed SHAP plot to interpret black-box model decisions, identified most important feature."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF Version"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "Education",
    "text": "Education\nUniversity of British Columbia (UBC) | Vancouver, BC\nM.S. in Data Science, Computational Linguistics | Aug 2025 - Present\n\nRelevant Coursework: Supervised Learning, Unsupervised Learning, Natural Language Processing, Machine Learning, Statistics.\nAcademic Focus: Examine the patterns in data and build models to predict the future.\n\nUniversity of California, Santa Cruz (UCSC) | Santa Cruz, CA\nB.A. in Business Management Economics | Sep 2019 - June 2022\n\nRelevant Coursework: Advanced Quantitative Analysis, Machine Learning Economics, Statistics, Security Market.\nAcademic Focus: Digging into the Economical patterns by using Machine Learning and Statistics."
  },
  {
    "objectID": "resume.html#technical-skills",
    "href": "resume.html#technical-skills",
    "title": "Resume",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\n\nCategory\nSkills\n\n\n\n\nLanguages\nPython (Advanced), R, SQL, Bash\n\n\nData Science\nPandas, NumPy, Scikit-Learn, PyTorch, HuggingFace\n\n\nNLP\nWord Embeddings, N-gram Models, Parsing Algorithms, NLTK, Spacy\n\n\nTools\nGit/GitHub, VS Code, Jupyter, Quarto, LaTeX\n\n\nPlatforms\nmacOS, Windows"
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "Experience",
    "text": "Experience\nCareer Mentorship Program | Mentee Jan 2026 - Present\n\nSelected for a professional mentorship program under industry veteran Jacob Oh.\nFocusing on software engineering best practices, career development in tech, and bridging academic concepts to industry standards."
  },
  {
    "objectID": "resume.html#capstone",
    "href": "resume.html#capstone",
    "title": "Resume",
    "section": "Capstone",
    "text": "Capstone\nTBD"
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html",
    "href": "posts/weather insurance purchasing prediction.html",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "",
    "text": "This project focuses on quantifying factors influencing Chinese farmers’ decisions to purchase weather insurance and predicting their purchasing probabilities. Using a dataset from Jiangxi, China, which includes 4,902 observations and 59 variables[cite: 30], the study navigates the challenges of predicting human economic behavior in a low-dimensional setting.\nBy implementing Lasso Cross-Validation and Random Forest algorithms, the project contrasts linear regularization methods with non-linear tree-based methods to identify significant determinants such as “Understanding” and “Social Network”.\n\n\n\nVariable Selection with Lasso: Utilized Lasso regularization to handle the bias-variance trade-off. The optimal \\(\\lambda\\) zeroed out 14 regressors, highlighting “Understanding” and “Network-related” variables as the most significant predictors.\nEnsemble Learning (Random Forest): Constructed a Random Forest model with 300 trees to capture non-linear relationships. Results revealed that unlike in the Lasso model, “Age” played a considerable role in purchasing decisions, while “Risk Averse” traits were surprisingly less important.\nPerformance Evaluation: Evaluated models using ROC Curves, Sensitivity/Specificity trade-offs, and \\(R^2\\). While the Lasso model achieved ~64% prediction correctness, the low \\(R^2\\) (&lt; 0.1) across both models highlighted the complexity of behavioral prediction and potential unobserved variables.\nData Cleaning & Bias Analysis: Processed raw data by handling NA values and removing variables with &gt;1000 missing entries. Conducted a critical analysis of data limitations, acknowledging potential biases from omitted variables like modern technology adoption."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#project-overview",
    "href": "posts/weather insurance purchasing prediction.html#project-overview",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "",
    "text": "This project focuses on quantifying factors influencing Chinese farmers’ decisions to purchase weather insurance and predicting their purchasing probabilities. Using a dataset from Jiangxi, China, which includes 4,902 observations and 59 variables[cite: 30], the study navigates the challenges of predicting human economic behavior in a low-dimensional setting.\nBy implementing Lasso Cross-Validation and Random Forest algorithms, the project contrasts linear regularization methods with non-linear tree-based methods to identify significant determinants such as “Understanding” and “Social Network”.\n\n\n\nVariable Selection with Lasso: Utilized Lasso regularization to handle the bias-variance trade-off. The optimal \\(\\lambda\\) zeroed out 14 regressors, highlighting “Understanding” and “Network-related” variables as the most significant predictors.\nEnsemble Learning (Random Forest): Constructed a Random Forest model with 300 trees to capture non-linear relationships. Results revealed that unlike in the Lasso model, “Age” played a considerable role in purchasing decisions, while “Risk Averse” traits were surprisingly less important.\nPerformance Evaluation: Evaluated models using ROC Curves, Sensitivity/Specificity trade-offs, and \\(R^2\\). While the Lasso model achieved ~64% prediction correctness, the low \\(R^2\\) (&lt; 0.1) across both models highlighted the complexity of behavioral prediction and potential unobserved variables.\nData Cleaning & Bias Analysis: Processed raw data by handling NA values and removing variables with &gt;1000 missing entries. Conducted a critical analysis of data limitations, acknowledging potential biases from omitted variables like modern technology adoption."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#paper-preview",
    "href": "posts/weather insurance purchasing prediction.html#paper-preview",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/weather insurance purchasing prediction.html#link-to-repo",
    "href": "posts/weather insurance purchasing prediction.html#link-to-repo",
    "title": "Weather Insurance Purchasing Prediction",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "posts/covid19 did analysis.html",
    "href": "posts/covid19 did analysis.html",
    "title": "Covid-19 DiD Analysis",
    "section": "",
    "text": "This project estimates the early policy effects on COVID-19 transmission and damages through four dependent variables: daily new cases, cumulative cases, daily deaths, and cumulative deaths. The data is provided by the World Health Organization and Our World in Data.\nTwo different Difference-in-Difference models were implemented to estimate the effects of Mandatory policies, Optional policies, and no-control policies. The analysis found that mandatory policies had the most significant impact on limiting and decreasing COVID-19 transmission and damages, followed by optional policies, while no-control policies were the least effective.\n\n\n\nDifference-in-Difference (DiD): Utilized DiD models to compare the impact of different policy stringency levels on infection and death rates over time.\nPolicy Categorization: Countries were categorized into three groups based on early policy stringency:\n\nMandatory Countries (e.g., China): Strict travel restrictions and enforced preventive measures.\nOptional Countries (e.g., Canada, Japan, UK): Advised but not strictly enforced measures.\nNo-control Countries (e.g., USA, India): Limited restrictions and guidance.\n\nStatistical Analysis: Analyzed the effects on new cases, cumulative cases, new deaths, and cumulative deaths using empiricial statistical methods."
  },
  {
    "objectID": "posts/covid19 did analysis.html#project-overview",
    "href": "posts/covid19 did analysis.html#project-overview",
    "title": "Covid-19 DiD Analysis",
    "section": "",
    "text": "This project estimates the early policy effects on COVID-19 transmission and damages through four dependent variables: daily new cases, cumulative cases, daily deaths, and cumulative deaths. The data is provided by the World Health Organization and Our World in Data.\nTwo different Difference-in-Difference models were implemented to estimate the effects of Mandatory policies, Optional policies, and no-control policies. The analysis found that mandatory policies had the most significant impact on limiting and decreasing COVID-19 transmission and damages, followed by optional policies, while no-control policies were the least effective.\n\n\n\nDifference-in-Difference (DiD): Utilized DiD models to compare the impact of different policy stringency levels on infection and death rates over time.\nPolicy Categorization: Countries were categorized into three groups based on early policy stringency:\n\nMandatory Countries (e.g., China): Strict travel restrictions and enforced preventive measures.\nOptional Countries (e.g., Canada, Japan, UK): Advised but not strictly enforced measures.\nNo-control Countries (e.g., USA, India): Limited restrictions and guidance.\n\nStatistical Analysis: Analyzed the effects on new cases, cumulative cases, new deaths, and cumulative deaths using empiricial statistical methods."
  },
  {
    "objectID": "posts/covid19 did analysis.html#paper-preview",
    "href": "posts/covid19 did analysis.html#paper-preview",
    "title": "Covid-19 DiD Analysis",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/covid19 did analysis.html#link-to-repo",
    "href": "posts/covid19 did analysis.html#link-to-repo",
    "title": "Covid-19 DiD Analysis",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tianhao Cao",
    "section": "",
    "text": "I am a student at the University of British Columbia (UBC), passionate about bridging the gap between Statistics and Computer Science.\nMy primary interests lie in Data Science, Machine Learning, and Natural Language Processing (NLP). I enjoy building tools that solve real-world problems—whether it’s digging underlying logics behind phenomena or constructing fundamental model/algorithm to predict the output.\n\nEducation\nUniversity of British Columbia | Vancouver, BC, Canada\n\nM.S. in Data Science, Computational Linguistics | August 2025 - Present\n\nUniversity of California, Santa Cruz | Santa Cruz, CA, United States\n\nB.A. in Business Management Economics | Sept 2019 - June 2022\n\n\n\nInterests\n\nProgramming: Python (Pandas, NumPy, ScikitLearn, PyTorch), R, SQL\nData Science: Machine Learning, Data Analaysis, NLP (Word Embeddings, Parsing)\nOther: Music Theory, Mathematics"
  },
  {
    "objectID": "posts/credit card default analysis.html#link-to-repo",
    "href": "posts/credit card default analysis.html#link-to-repo",
    "title": "Credit Card Default Analysis",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "posts/sentiment analysis.html",
    "href": "posts/sentiment analysis.html",
    "title": "Sentiment Analysis on Yelp Reviews",
    "section": "",
    "text": "This project focuses on performing sentiment analysis on the Yelp Review dataset, classifying reviews into 1 to 5 stars. The analysis explores various modeling approaches, ranging from traditional machine learning baselines to deep learning architectures.\nBy implementing and comparing TF-IDF with Logistic Regression, Continuous Bag of Words (CBOW), and Bidirectional LSTM models, the study evaluates the effectiveness of different techniques in handling text classification tasks.\n\n\n\nExploratory Data Analysis (EDA): Analyzed the distribution of ratings and review lengths using Altair. The dataset was found to be balanced across different rating classes.\nBaseline Modeling: Established a baseline using TF-IDF Vectorization and Logistic Regression, achieving a Macro F1 score of 0.5936, setting a strong benchmark for subsequent models.\nDeep Learning Architectures:\n\nCBOW (Continuous Bag of Words): Implemented a custom CBOW model using Spacy word embeddings.\nBi-LSTM (Bidirectional Long Short-Term Memory): Constructed a Bi-LSTM model to capture sequential dependencies in the text. The model outperformed the CBOW approach, achieving a Validation F1 score of approximately 0.557.\n\nHyperparameter Tuning: Conducted experiments with different architectures (e.g., varying hidden sizes and number of layers) to optimize the LSTM model performance."
  },
  {
    "objectID": "posts/sentiment analysis.html#project-overview",
    "href": "posts/sentiment analysis.html#project-overview",
    "title": "Sentiment Analysis on Yelp Reviews",
    "section": "",
    "text": "This project focuses on performing sentiment analysis on the Yelp Review dataset, classifying reviews into 1 to 5 stars. The analysis explores various modeling approaches, ranging from traditional machine learning baselines to deep learning architectures.\nBy implementing and comparing TF-IDF with Logistic Regression, Continuous Bag of Words (CBOW), and Bidirectional LSTM models, the study evaluates the effectiveness of different techniques in handling text classification tasks.\n\n\n\nExploratory Data Analysis (EDA): Analyzed the distribution of ratings and review lengths using Altair. The dataset was found to be balanced across different rating classes.\nBaseline Modeling: Established a baseline using TF-IDF Vectorization and Logistic Regression, achieving a Macro F1 score of 0.5936, setting a strong benchmark for subsequent models.\nDeep Learning Architectures:\n\nCBOW (Continuous Bag of Words): Implemented a custom CBOW model using Spacy word embeddings.\nBi-LSTM (Bidirectional Long Short-Term Memory): Constructed a Bi-LSTM model to capture sequential dependencies in the text. The model outperformed the CBOW approach, achieving a Validation F1 score of approximately 0.557.\n\nHyperparameter Tuning: Conducted experiments with different architectures (e.g., varying hidden sizes and number of layers) to optimize the LSTM model performance."
  },
  {
    "objectID": "posts/sentiment analysis.html#link-to-repo",
    "href": "posts/sentiment analysis.html#link-to-repo",
    "title": "Sentiment Analysis on Yelp Reviews",
    "section": "Link to repo",
    "text": "Link to repo\nGithub Repository"
  },
  {
    "objectID": "posts/notebook/sentiment_analysis.html",
    "href": "posts/notebook/sentiment_analysis.html",
    "title": "Please write code to develop you system. More details are in Lab4.ipynb.",
    "section": "",
    "text": "# all the necessary imports\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch import optim\nimport torch\nimport pandas as pd\nimport numpy as np\nimport spacy\nimport altair as alt\nimport gdown\nnlp = spacy.load(\"en_core_web_sm\")\nalt.data_transformers.enable(\"vegafusion\")\n\nDataTransformerRegistry.enable('vegafusion')\n\n\n\n# set the seed\nmanual_seed = 572\ntorch.manual_seed(manual_seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\nprint(device)\n\ncuda\n\n\nYou can adap these two functions for your model.\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\ndef train(loader):\n    model.train()\n    total_loss = 0.0\n    # iterate throught the data loader\n    num_sample = 0\n    for batch in loader:\n        # load the current batch\n        batch_input = batch.review\n        batch_output = batch.label\n\n        batch_input = batch_input.to(device)\n        batch_output = batch_output.to(device)\n        # forward propagation\n        # pass the data through the model\n        model_outputs = model(batch_input)\n        # compute the loss\n        cur_loss = criterion(model_outputs, batch_output)\n        total_loss += cur_loss.cpu().item()\n\n        # backward propagation (compute the gradients and update the model)\n        # clear the buffer\n        optimizer.zero_grad()\n        # compute the gradients\n        cur_loss.backward()\n        # update the weights\n        optimizer.step()\n\n        num_sample += batch_output.shape[0]\n\n    return total_loss/num_sample\n\n# evaluation logic based on classification accuracy\ndef evaluate(loader):\n    model.eval()\n    all_pred=[]\n    all_label = []\n    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n        for batch in loader:\n             # load the current batch\n            batch_input = batch.review\n            batch_output = batch.label\n\n            batch_input = batch_input.to(device)\n            # forward propagation\n            # pass the data through the model\n            model_outputs = model(batch_input)\n            # identify the predicted class for each example in the batch\n            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n            # put all the true labels and predictions to two lists\n            all_pred.extend(predicted)\n            all_label.extend(batch_output.cpu())\n\n    accuracy = accuracy_score(all_label, all_pred)\n    f1score = f1_score(all_label, all_pred, average='macro')\n    return accuracy,f1score\n\n\n# funtion for save prediction\ndef out_prediction(first_name, last_name, prediction_list):\n    \"\"\"\n    out_prediction takes three input varibles: first_name, last_name, prediction_list\n    &lt;first_name&gt;, string, your first name, e.g., Tom\n    &lt;last_name&gt;, string, your last name, e.g., Smith\n    &lt;prediction_list&gt;, list of string which includes all your predications of TEST samples\n                        e.g., ['1star','5star','3star']\n\n    Generate a file is named with &lt;yourfirstname&gt;_&lt;yourlastname&gt;_PRED.txt in current directory\n    \"\"\"\n    output_file = open(\"{}_{}_PRED.txt\".format(first_name,last_name),'w')\n    for item in prediction_list:\n        output_file.write(item+\"\\n\")\n    output_file.close()\n\n\nPlease write code to develop you system. More details are in Lab4.ipynb.\n\n# Data download to colab working directory\nurl = f'https://drive.google.com/drive/folders/1zF5s4KRMxpr3OvUY8o_d0xv_YlaZSCsj?usp=drive_link'\ngdown.download_folder(url, quiet = False, use_cookies = False)\n\nRetrieving folder contents\n\n\nRetrieving folder 1miNtgEGuJ6F-2cGJ5crifLQUbWnb-wJl yelp_review\nRetrieving folder 10gkkUq4rNU1HWSUr6GOQ0iwprA5gaV6e .ipynb_checkpoints\nProcessing file 1FrYOgXiu-pVX_TQ57pSUtpwHeu1R4J0k EXAMPLE_GOLD-checkpoint.txt\nProcessing file 1oCDq8R-xBdwcJBr_2KLpzg6WVgQN_Re9 EXAMPLE_PRED_result-checkpoint.txt\nProcessing file 1MpSkPKP6T043NDhpS4zxi4JNgnzB_-Zg EXAMPLE_GOLD.txt\nProcessing file 1ztc0m5FuX1CaTm72iVIZpP_eRxQpl0Wt EXAMPLE_PRED_result.txt\nProcessing file 1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r Scorer.py\nProcessing file 1crLuR8fhJ89RKEVut-UjgIyOz6-y5eh4 test.tsv\nProcessing file 10qVnIg3rvEXlcEA9hynHTKoYDURLQN4v train.tsv\nProcessing file 1zozCGPs0XiJAPl1Il5cSWl9p0Aqv8YgO val.tsv\nProcessing file 1Vr4eJVM9kIK38ZONzamuIrQKf_gi3pw- .Rhistory\nProcessing file 19QypIq9BjPB_WhGIsT8Zrr658VV6uQHC readme.md\n\n\nRetrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom: https://drive.google.com/uc?id=1FrYOgXiu-pVX_TQ57pSUtpwHeu1R4J0k\nTo: /content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_GOLD-checkpoint.txt\n100%|██████████| 21.0k/21.0k [00:00&lt;00:00, 29.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1oCDq8R-xBdwcJBr_2KLpzg6WVgQN_Re9\nTo: /content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_PRED_result-checkpoint.txt\n100%|██████████| 149/149 [00:00&lt;00:00, 531kB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1MpSkPKP6T043NDhpS4zxi4JNgnzB_-Zg\nTo: /content/data/yelp_review/EXAMPLE_GOLD.txt\n100%|██████████| 21.0k/21.0k [00:00&lt;00:00, 41.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1ztc0m5FuX1CaTm72iVIZpP_eRxQpl0Wt\nTo: /content/data/yelp_review/EXAMPLE_PRED_result.txt\n100%|██████████| 149/149 [00:00&lt;00:00, 638kB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r\nFrom (redirected): https://drive.google.com/uc?id=1jjiGoOLJLlucSlJtb_EJ0Vo2xIUaMN2r&confirm=t&uuid=d1b9cde2-621d-49fa-b05b-920687536023\nTo: /content/data/yelp_review/Scorer.py\n100%|██████████| 3.35k/3.35k [00:00&lt;00:00, 10.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1crLuR8fhJ89RKEVut-UjgIyOz6-y5eh4\nTo: /content/data/yelp_review/test.tsv\n100%|██████████| 2.63M/2.63M [00:00&lt;00:00, 13.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=10qVnIg3rvEXlcEA9hynHTKoYDURLQN4v\nTo: /content/data/yelp_review/train.tsv\n100%|██████████| 21.3M/21.3M [00:00&lt;00:00, 68.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1zozCGPs0XiJAPl1Il5cSWl9p0Aqv8YgO\nTo: /content/data/yelp_review/val.tsv\n100%|██████████| 2.67M/2.67M [00:00&lt;00:00, 20.1MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Vr4eJVM9kIK38ZONzamuIrQKf_gi3pw-\nTo: /content/data/.Rhistory\n0.00B [00:00, ?B/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=19QypIq9BjPB_WhGIsT8Zrr658VV6uQHC\nTo: /content/data/readme.md\n100%|██████████| 98.0/98.0 [00:00&lt;00:00, 418kB/s]\nDownload completed\n\n\n['/content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_GOLD-checkpoint.txt',\n '/content/data/yelp_review/.ipynb_checkpoints/EXAMPLE_PRED_result-checkpoint.txt',\n '/content/data/yelp_review/EXAMPLE_GOLD.txt',\n '/content/data/yelp_review/EXAMPLE_PRED_result.txt',\n '/content/data/yelp_review/Scorer.py',\n '/content/data/yelp_review/test.tsv',\n '/content/data/yelp_review/train.tsv',\n '/content/data/yelp_review/val.tsv',\n '/content/data/.Rhistory',\n '/content/data/readme.md']\n\n\n\n### Import for EDA\ntrain_set = pd.read_csv('data/yelp_review/train.tsv', sep = '\\t')\ndev_set = pd.read_csv('data/yelp_review/val.tsv', sep = '\\t')\ntest_set = pd.read_csv('data/yelp_review/test.tsv', sep = '\\t')\n\ntrain_set.head()\n\n\n    \n\n\n\n\n\n\ncontent\nrating\n\n\n\n\n0\nThere are some restaurants that you don't want...\n4star\n\n\n1\nLucky for us there was no wait unlike other ti...\n4star\n\n\n2\nWorst ever Michelin restaurant I have ever bee...\n1star\n\n\n3\nCame here today to celebrate my birthdays with...\n4star\n\n\n4\nThis, is where hipsters go to get Caribbean fo...\n2star\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\nEDA\n\n### EDA training set\nimport copy\n\ntrain_eda = train_set.copy()\ntrain_eda['review_length'] = train_set['content'].apply(lambda x: len(str(x).split()))\n\n# Rating Plot\nratings = alt.Chart(train_eda).mark_bar().encode(\n    x = alt.X('rating').sort(['1star', '2star', '3star', '4star', '5star']).title('Rating'),\n    y = alt.Y('count()').title('Number of Reviews'),\n    color = alt.Color('rating').legend(None),\n    tooltip = ['rating', 'count()']\n).properties(\n    title = 'Distribution of Ratings',\n    width = 300,\n    height = 300\n)\n\n# Length Plot\nlength = alt.Chart(train_eda).mark_bar().encode(\n    x=alt.X('review_length', bin=alt.Bin(maxbins=60), title='Word Count per Review'),\n    y=alt.Y('count()', title='Frequency'),\n    color = alt.Color('rating').legend(None),\n    tooltip=['count()']\n).properties(\n    title='Distribution of Review Lengths',\n    width=400,\n    height=300\n)\n\nfinal_chart = ratings | length\nfinal_chart\n\n\n\n\n\n\n\nFrom EDA, we know that the classes are balanced, and it seems like all the ratings are in proportion to the length of the comments.\n\n\nBaseline\nFor baseline, I will use tfidf vectorizer and logistic regression, it is fast and provide a decent standard baseline score for my further models.\nThe baseline Macro F1 Score = 59.36%\n\n### baseline\n\n## import\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.pipeline import make_pipeline\n\n\n## data split\n\nX_train = train_set['content']\ny_train = train_set['rating']\n\nX_dev = dev_set['content']\ny_dev = dev_set['rating']\n\nprint(len(X_train), len(X_dev))\n\n28000 3500\n\n\n\n## Pipeline\nbaseline = make_pipeline(\n    TfidfVectorizer(\n        stop_words = 'english',\n        ngram_range = (1, 2)\n    ),\n\n    LogisticRegression(\n        solver = 'liblinear',\n        C = 1.0\n    )\n)\n\n# baseline train & pred\nbaseline.fit(X_train, y_train)\ny_pred_baseline = baseline.predict(X_dev)\n\n# calculate macro f1\nf1_baseline = f1_score(y_dev, y_pred_baseline, average = 'macro')\n\n\nprint(f'The Macro F1 score of baseline with tfidf and logistic is: {f1_baseline}')\n\nThe Macro F1 score of baseline with tfidf and logistic is: 0.5936185962662195\n\n\n\n\nCBOW\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_set.rating)\n\ntrain_y = label_encoder.transform(train_set.rating)\ndev_y = label_encoder.transform(dev_set.rating)\n\n\n# Vocabulary\nfrom collections import defaultdict, Counter\n\nvocabulary = Counter()\nfor sentence in train_set.content:\n    tokens = sentence.lower().split()\n    vocabulary.update(tokens)\n\n\nfrom collections import defaultdict, Counter\n\n## Create a w2i\ndef BuildWord2i(contents):\n    counter = Counter()\n\n    for content in contents:\n        tokens = str(content).lower().split()\n        counter.update(tokens)\n\n    word2i = {}\n    word2i['&lt;PAD&gt;'] = 0\n    word2i['&lt;UNK&gt;'] = 1\n\n    idx = 2\n    for word, count in counter.items():\n        word2i[word] = idx\n        idx += 1\n\n    return word2i\n\n## get w2i\nw2i = BuildWord2i(train_set['content'])\n\n\n## Get the embedding matrix from spacy\ndef BuildEmbeddingMatrix(word2i, vocab_size, emb_dim = 300):\n    weights_matrix = np.random.normal(scale = 0.6, size = (vocab_size, emb_dim))\n\n    weights_matrix[w2i['&lt;PAD&gt;']] = np.zeros((emb_dim,))\n    count = 0\n    for word, i in w2i.items():\n        if word in nlp.vocab and nlp.vocab[word].has_vector:\n            weights_matrix[i] = nlp.vocab[word].vector\n            found_count += 1\n    return torch.tensor(weights_matrix, dtype = torch.float32)\n\n## create embedding weights from spacy\nEMBEDDING_DIM = 300\nembedding_weights = BuildEmbeddingMatrix(w2i, len(w2i), EMBEDDING_DIM)\nembedding_weights.shape\n\ntorch.Size([106121, 300])\n\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\ndef create_data_loader(df, y, w2i, batch_size=32, shuffle=True):\n    pad_token_id = w2i.get('&lt;PAD&gt;', 0)\n    unk_token_id = w2i.get('&lt;UNK&gt;', 0)\n\n    def text_to_indices(text):\n        tokens = text.split()\n        return [w2i.get(token, unk_token_id) for token in tokens]\n\n    indices_list = [torch.tensor(text_to_indices(text)) for text in df['content']]\n    padded_inputs = pad_sequence(indices_list, batch_first=True, padding_value=pad_token_id)\n\n    labels = torch.tensor(y, dtype=torch.long)\n\n    dataset = TensorDataset(padded_inputs, labels)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n    return loader\n\nBATCH_SIZE = 64\ntrain_loader = create_data_loader(train_set, train_y, w2i, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = create_data_loader(dev_set, dev_y,  w2i, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# CBOW originated from lab 3\nHIDDEN_SIZE = 100\nclass CBOW(nn.Module):\n    def __init__(self, weights_matrix, num_classes, dropout_prob, padding_idx):\n        super().__init__()\n        self.embedding = nn.Embedding.from_pretrained(weights_matrix, padding_idx = padding_idx)\n        self.embedding_dim = weights_matrix.shape[1]\n\n        self.linear1 = nn.Linear(self.embedding_dim, HIDDEN_SIZE)\n        self.dropout = nn.Dropout(p = dropout_prob)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(HIDDEN_SIZE, num_classes)\n\n        self.padding_idx = padding_idx\n\n    def forward(self, x):\n            non_pad_mask = (x != self.padding_idx)\n            lengths = non_pad_mask.sum(dim=1).float().clamp(min=1).unsqueeze(1)\n\n            embedded = self.embedding(x)\n            x = torch.sum(embedded, dim=1) / lengths\n\n            x = self.linear1(x)\n            x = self.dropout(x)\n            x = self.relu(x)\n            x = self.linear2(x)\n            return x\n\n\n\npad_idx = w2i['&lt;PAD&gt;']\n\nmodel = CBOW(\n    weights_matrix = embedding_weights,\n    num_classes = 5,\n    dropout_prob = 0.5,\n    padding_idx = pad_idx\n).to(device)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\nNUM_EPOCHS = 3\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0\n\n    with torch.no_grad():\n        for inputs, labels in dev_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(dev_loader)\n    val_accuracy = 100 * correct / total\n\n    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], '\n          f'Train Loss: {avg_train_loss:.4f}, '\n          f'Val Loss: {avg_val_loss:.4f}, '\n          f'Val Acc: {val_accuracy:.2f}%')\n\nprint(\"Finish processing\")\n\nEpoch [1/3], Train Loss: 1.4408, Val Loss: 1.3469, Val Acc: 41.74%\nEpoch [2/3], Train Loss: 1.3760, Val Loss: 1.3383, Val Acc: 42.23%\nEpoch [3/3], Train Loss: 1.3639, Val Loss: 1.3268, Val Acc: 42.54%\nFinish processing\n\n\n\n\nLSTM\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nclass LSTM(nn.Module):\n    def __init__(self, weights_matrix, num_classes, hidden_size = 256, num_layers = 1, dropout_prob = 0.5, padding_idx = 0):\n        super().__init__()\n\n        weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n        self.embedding = nn.Embedding.from_pretrained(\n            weights_tensor,\n            padding_idx=padding_idx\n        )\n        self.emb_dim = weights_matrix.shape[1]\n\n        self.lstm = nn.LSTM(\n            input_size=self.emb_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=dropout_prob if num_layers &gt; 1 else 0\n        )\n\n        self.fc = nn.Linear(hidden_size * 2, num_classes)\n\n        self.dropout = nn.Dropout(dropout_prob)\n        self.padding_idx = padding_idx\n\n    def forward(self, x):\n        lengths = (x != self.padding_idx).sum(dim=1).cpu()\n\n        embeds = self.embedding(x) # [Batch, Seq, Emb]\n        packed_input = pack_padded_sequence(embeds, lengths, batch_first=True, enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_input)\n\n        cat_hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n\n        output = self.dropout(cat_hidden)\n        output = self.fc(output)\n\n        return output\n\n\nHIDDEN_SIZE = 128\nNUM_LAYERS = 2\nDROPOUT = 0.5\nPAD_IDX = w2i['&lt;PAD&gt;']\nNUM_CLASSES = 5\n\nmodel = LSTM(\n    weights_matrix=embedding_weights,\n    num_classes=NUM_CLASSES,\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LAYERS,\n    dropout_prob=DROPOUT,\n    padding_idx=PAD_IDX\n).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n\n/tmp/ipython-input-2886251409.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n\n\n\ntrain_loss = float('inf')\nbest_val_loss = float('inf')\npatience = 3\ntrigger_times = 0\nepoch = 0\nTHRESHOLD = 0.01\nMAX_EPOCHS = 100\n\nwhile train_loss &gt; THRESHOLD and epoch &lt; MAX_EPOCHS:\n    epoch += 1\n    model.train()\n    total_loss = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    train_loss = avg_train_loss\n\n    model.eval()\n    val_loss = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in dev_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = val_loss / len(dev_loader)\n\n    val_f1 = f1_score(all_labels, all_preds, average='macro')\n\n    if avg_val_loss &lt; best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n        print(f\"Save best model (F1: {val_f1:.4f})\")\n        trigger_times = 0\n    else:\n        trigger_times += 1\n        print(f\"Val Loss ({trigger_times}/{patience})\")\n\n        if trigger_times &gt;= patience:\n            print(\"Early stop triggers\")\n            break\n\n    print(f'Epoch [{epoch}], '\n          f'Train Loss: {avg_train_loss:.4f}, '\n          f'Val Loss: {avg_val_loss:.4f}, '\n          f'Val F1 (Macro): {val_f1:.4f}')\n\nprint(\"Finish training\")\nmodel.load_state_dict(torch.load('best_model.pth'))\n\nSave best model (F1: 0.4521)\nEpoch [1], Train Loss: 1.4031, Val Loss: 1.2327, Val F1 (Macro): 0.4521\nVal Loss (1/3)\nEpoch [2], Train Loss: 1.2334, Val Loss: 1.2392, Val F1 (Macro): 0.4346\nSave best model (F1: 0.4686)\nEpoch [3], Train Loss: 1.1421, Val Loss: 1.1874, Val F1 (Macro): 0.4686\nSave best model (F1: 0.5289)\nEpoch [4], Train Loss: 1.0361, Val Loss: 1.0636, Val F1 (Macro): 0.5289\nSave best model (F1: 0.5310)\nEpoch [5], Train Loss: 0.9297, Val Loss: 1.0446, Val F1 (Macro): 0.5310\nVal Loss (1/3)\nEpoch [6], Train Loss: 0.8368, Val Loss: 1.0540, Val F1 (Macro): 0.5501\nVal Loss (2/3)\nEpoch [7], Train Loss: 0.7459, Val Loss: 1.0646, Val F1 (Macro): 0.5570\nVal Loss (3/3)\nEarly stop triggers\nFinish training\n\n\n&lt;All keys matched successfully&gt;\n\n\n\n# Hyper parameter tunning\n\nHIDDEN_SIZE = 256\nNUM_LAYERS = 3\nDROPOUT = 0.5\nPAD_IDX = w2i['&lt;PAD&gt;']\nNUM_CLASSES = 5\n\nmodel_tuned = LSTM(\n    weights_matrix=embedding_weights,\n    num_classes=NUM_CLASSES,\n    hidden_size=HIDDEN_SIZE,\n    num_layers=NUM_LAYERS,\n    dropout_prob=DROPOUT,\n    padding_idx=PAD_IDX\n).to(device)\n\noptimizer = torch.optim.Adam(model_tuned.parameters(), lr = 0.001)\n\n/tmp/ipython-input-2886251409.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  weights_tensor = torch.tensor(weights_matrix, dtype=torch.float)\n\n\n\ntrain_loss = float('inf')\nbest_val_loss = float('inf')\npatience = 3\ntrigger_times = 0\nepoch = 0\nTHRESHOLD = 0.01\nMAX_EPOCHS = 100\n\nwhile train_loss &gt; THRESHOLD and epoch &lt; MAX_EPOCHS:\n    epoch += 1\n    model_tuned.train()\n    total_loss = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model_tuned(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    train_loss = avg_train_loss\n\n    model_tuned.eval()\n    val_loss = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in dev_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model_tuned(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_val_loss = val_loss / len(dev_loader)\n\n    val_f1 = f1_score(all_labels, all_preds, average='macro')\n\n    if avg_val_loss &lt; best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model_tuned.state_dict(), 'best_model_tuned.pth')\n        print(f\"Save best model_tuned (F1: {val_f1:.4f})\")\n        trigger_times = 0\n    else:\n        trigger_times += 1\n        print(f\"Val Loss ({trigger_times}/{patience})\")\n\n        if trigger_times &gt;= patience:\n            print(\"Early stop triggers\")\n            break\n\n    print(f'Epoch [{epoch}], '\n          f'Train Loss: {avg_train_loss:.4f}, '\n          f'Val Loss: {avg_val_loss:.4f}, '\n          f'Val F1 (Macro): {val_f1:.4f}')\n\nprint(\"Finish training\")\nmodel_tuned.load_state_dict(torch.load('best_model_tuned.pth'))\n\nSave best model_tuned (F1: 0.3612)\nEpoch [1], Train Loss: 1.4040, Val Loss: 1.2835, Val F1 (Macro): 0.3612\nSave best model_tuned (F1: 0.4822)\nEpoch [2], Train Loss: 1.2019, Val Loss: 1.1576, Val F1 (Macro): 0.4822\nSave best model_tuned (F1: 0.5128)\nEpoch [3], Train Loss: 1.0639, Val Loss: 1.0784, Val F1 (Macro): 0.5128\nSave best model_tuned (F1: 0.5407)\nEpoch [4], Train Loss: 0.9518, Val Loss: 1.0268, Val F1 (Macro): 0.5407\nVal Loss (1/3)\nEpoch [5], Train Loss: 0.8555, Val Loss: 1.0524, Val F1 (Macro): 0.5452\nVal Loss (2/3)\nEpoch [6], Train Loss: 0.7450, Val Loss: 1.0955, Val F1 (Macro): 0.5398\nVal Loss (3/3)\nEarly stop triggers\nFinish training\n\n\n&lt;All keys matched successfully&gt;\n\n\n\n\npredict on the test set\n\ntest_set['rating'] = '1star'\ntest_loader = create_data_loader(test_set, w2i = w2i, y = label_encoder.transform(test_set.rating), batch_size = 64)\n\n\ndef get_preds(model, loader):\n    model.eval()\n    all_preds = []\n\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            preds = predicted.cpu().numpy()\n\n            all_preds.extend(['star' + str(x) for x in preds])\n\n    return all_preds\n\ntest_preds = get_preds(model_tuned, test_loader)\ntest_preds\n\n['star3',\n 'star2',\n 'star2',\n 'star2',\n 'star2',\n 'star0',\n 'star3',\n 'star1',\n 'star0',\n 'star0',\n 'star0',\n 'star1',\n 'star3',\n 'star4',\n 'star3',\n 'star3',\n 'star2',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star4',\n 'star2',\n 'star4',\n 'star1',\n 'star3',\n 'star4',\n 'star1',\n 'star4',\n 'star0',\n 'star0',\n 'star4',\n 'star4',\n 'star3',\n 'star3',\n 'star3',\n 'star2',\n 'star2',\n 'star0',\n 'star4',\n 'star2',\n 'star4',\n 'star1',\n 'star3',\n 'star4',\n 'star2',\n 'star0',\n 'star0',\n 'star0',\n 'star0',\n 'star4',\n 'star3',\n 'star3',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star1',\n 'star1',\n 'star1',\n 'star0',\n 'star1',\n 'star1',\n 'star1',\n 'star1',\n 'star1',\n 'star2',\n 'star3',\n 'star4',\n 'star2',\n 'star1',\n 'star1',\n 'star1',\n 'star0',\n 'star4',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star0',\n 'star1',\n 'star2',\n 'star1',\n 'star4',\n 'star0',\n 'star1',\n 'star0',\n 'star3',\n 'star3',\n 'star4',\n 'star0',\n 'star3',\n 'star0',\n 'star1',\n 'star0',\n 'star4',\n 'star3',\n 'star0',\n 'star0',\n 'star4',\n 'star1',\n 'star1',\n 'star3',\n 'star1',\n 'star0',\n 'star1',\n 'star2',\n 'star1',\n 'star3',\n 'star4',\n 'star0',\n 'star4',\n 'star2',\n 'star4',\n 'star3',\n 'star1',\n 'star3',\n 'star4',\n 'star2',\n 'star0',\n 'star1',\n 'star4',\n 'star1',\n 'star3',\n 'star0',\n 'star4',\n 'star2',\n 'star4',\n 'star2',\n 'star3',\n 'star1',\n 'star4',\n 'star2',\n 'star4',\n 'star3',\n 'star0',\n 'star4',\n 'star3',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star1',\n 'star3',\n 'star2',\n 'star4',\n 'star0',\n 'star0',\n 'star2',\n 'star4',\n 'star3',\n 'star3',\n 'star4',\n 'star0',\n 'star2',\n 'star4',\n 'star3',\n 'star4',\n 'star4',\n 'star1',\n 'star4',\n 'star3',\n 'star4',\n 'star3',\n 'star1',\n 'star3',\n 'star1',\n 'star4',\n 'star3',\n 'star4',\n 'star0',\n 'star2',\n 'star3',\n 'star1',\n 'star1',\n 'star2',\n 'star3',\n 'star1',\n 'star1',\n 'star1',\n 'star2',\n 'star4',\n 'star4',\n 'star2',\n 'star4',\n 'star1',\n 'star2',\n 'star2',\n 'star0',\n 'star1',\n 'star1',\n 'star3',\n 'star3',\n 'star4',\n 'star2',\n 'star0',\n 'star1',\n 'star1',\n 'star1',\n 'star1',\n 'star2',\n 'star4',\n 'star0',\n 'star2',\n 'star3',\n 'star3',\n 'star0',\n 'star4',\n 'star1',\n 'star1',\n 'star1',\n 'star0',\n 'star2',\n 'star3',\n 'star0',\n 'star1',\n 'star3',\n 'star1',\n 'star3',\n 'star3',\n 'star3',\n 'star1',\n 'star1',\n 'star2',\n 'star1',\n 'star4',\n 'star3',\n 'star4',\n 'star4',\n 'star4',\n 'star0',\n 'star0',\n 'star2',\n 'star3',\n 'star0',\n 'star4',\n 'star0',\n 'star4',\n 'star3',\n 'star4',\n 'star0',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star1',\n 'star0',\n 'star3',\n 'star1',\n 'star1',\n 'star2',\n 'star3',\n 'star3',\n 'star1',\n 'star0',\n 'star4',\n 'star1',\n 'star0',\n 'star3',\n 'star2',\n 'star0',\n 'star4',\n 'star2',\n 'star4',\n 'star4',\n 'star1',\n 'star1',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star4',\n 'star4',\n 'star1',\n 'star4',\n 'star4',\n 'star2',\n 'star4',\n 'star2',\n 'star3',\n 'star0',\n 'star3',\n 'star0',\n 'star2',\n 'star4',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star3',\n 'star0',\n 'star4',\n 'star1',\n 'star1',\n 'star2',\n 'star4',\n 'star4',\n 'star2',\n 'star2',\n 'star4',\n 'star2',\n 'star2',\n 'star3',\n 'star2',\n 'star2',\n 'star3',\n 'star1',\n 'star2',\n 'star3',\n 'star1',\n 'star3',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star0',\n 'star1',\n 'star0',\n 'star0',\n 'star0',\n 'star1',\n 'star0',\n 'star1',\n 'star1',\n 'star3',\n 'star1',\n 'star3',\n 'star1',\n 'star0',\n 'star2',\n 'star0',\n 'star3',\n 'star3',\n 'star0',\n 'star1',\n 'star2',\n 'star4',\n 'star2',\n 'star0',\n 'star2',\n 'star4',\n 'star3',\n 'star0',\n 'star3',\n 'star3',\n 'star3',\n 'star2',\n 'star2',\n 'star0',\n 'star0',\n 'star3',\n 'star4',\n 'star4',\n 'star2',\n 'star4',\n 'star1',\n 'star1',\n 'star4',\n 'star0',\n 'star3',\n 'star4',\n 'star2',\n 'star2',\n 'star0',\n 'star4',\n 'star1',\n 'star3',\n 'star0',\n 'star3',\n 'star1',\n 'star0',\n 'star4',\n 'star3',\n 'star3',\n 'star0',\n 'star1',\n 'star2',\n 'star1',\n 'star2',\n 'star2',\n 'star0',\n 'star2',\n 'star1',\n 'star4',\n 'star0',\n 'star1',\n 'star0',\n 'star0',\n 'star4',\n 'star0',\n 'star1',\n 'star4',\n 'star4',\n 'star2',\n 'star2',\n 'star3',\n 'star2',\n 'star2',\n 'star3',\n 'star0',\n 'star3',\n 'star1',\n 'star0',\n 'star0',\n 'star2',\n 'star2',\n 'star4',\n 'star2',\n 'star1',\n 'star2',\n 'star1',\n 'star3',\n 'star4',\n 'star4',\n 'star3',\n 'star1',\n 'star0',\n 'star4',\n 'star0',\n 'star0',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star1',\n 'star4',\n 'star3',\n 'star3',\n 'star0',\n 'star4',\n 'star2',\n 'star3',\n 'star0',\n 'star3',\n 'star4',\n 'star2',\n 'star3',\n 'star0',\n 'star1',\n 'star1',\n 'star3',\n 'star0',\n 'star1',\n 'star3',\n 'star2',\n 'star2',\n 'star0',\n 'star4',\n 'star4',\n 'star2',\n 'star2',\n 'star3',\n 'star0',\n 'star3',\n 'star0',\n 'star3',\n 'star2',\n 'star0',\n 'star3',\n 'star1',\n 'star3',\n 'star4',\n 'star3',\n 'star1',\n 'star0',\n 'star2',\n 'star4',\n 'star3',\n 'star4',\n 'star0',\n 'star3',\n 'star2',\n 'star3',\n 'star2',\n 'star4',\n 'star4',\n 'star3',\n 'star3',\n 'star1',\n 'star1',\n 'star2',\n 'star0',\n 'star3',\n 'star2',\n 'star0',\n 'star2',\n 'star4',\n 'star1',\n 'star1',\n 'star4',\n 'star1',\n 'star0',\n 'star4',\n 'star2',\n 'star1',\n 'star0',\n 'star3',\n 'star3',\n 'star2',\n 'star1',\n 'star2',\n 'star3',\n 'star0',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star3',\n 'star2',\n 'star2',\n 'star1',\n 'star3',\n 'star1',\n 'star0',\n 'star1',\n 'star0',\n 'star3',\n 'star0',\n 'star1',\n 'star3',\n 'star4',\n 'star2',\n 'star2',\n 'star3',\n 'star1',\n 'star0',\n 'star1',\n 'star0',\n 'star2',\n 'star1',\n 'star0',\n 'star0',\n 'star4',\n 'star0',\n 'star1',\n 'star3',\n 'star1',\n 'star2',\n 'star2',\n 'star4',\n 'star2',\n 'star1',\n 'star3',\n 'star4',\n 'star3',\n 'star2',\n 'star3',\n 'star1',\n 'star1',\n 'star2',\n 'star3',\n 'star3',\n 'star0',\n 'star0',\n 'star4',\n 'star1',\n 'star0',\n 'star2',\n 'star1',\n 'star0',\n 'star0',\n 'star3',\n 'star3',\n 'star3',\n 'star3',\n 'star3',\n 'star1',\n 'star3',\n 'star2',\n 'star1',\n 'star2',\n 'star3',\n 'star2',\n 'star1',\n 'star3',\n 'star3',\n 'star0',\n 'star2',\n 'star0',\n 'star2',\n 'star2',\n 'star1',\n 'star1',\n 'star2',\n 'star3',\n 'star2',\n 'star0',\n 'star3',\n 'star3',\n 'star0',\n 'star2',\n 'star4',\n 'star1',\n 'star4',\n 'star4',\n 'star4',\n 'star2',\n 'star1',\n 'star2',\n 'star4',\n 'star0',\n 'star0',\n 'star0',\n 'star0',\n 'star4',\n 'star1',\n 'star4',\n 'star0',\n 'star0',\n 'star4',\n 'star4',\n 'star1',\n 'star0',\n 'star3',\n 'star4',\n 'star0',\n 'star3',\n 'star3',\n 'star3',\n 'star0',\n 'star3',\n 'star3',\n 'star3',\n 'star3',\n 'star3',\n 'star4',\n 'star3',\n 'star2',\n 'star3',\n 'star1',\n 'star4',\n 'star2',\n 'star2',\n 'star1',\n 'star0',\n 'star1',\n 'star1',\n 'star1',\n 'star0',\n 'star3',\n 'star1',\n 'star2',\n 'star1',\n 'star0',\n 'star2',\n 'star2',\n 'star2',\n 'star1',\n 'star1',\n 'star3',\n 'star1',\n 'star1',\n 'star4',\n 'star4',\n 'star0',\n 'star3',\n 'star4',\n 'star1',\n 'star0',\n 'star1',\n 'star4',\n 'star4',\n 'star3',\n 'star0',\n 'star4',\n 'star3',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star2',\n 'star3',\n 'star4',\n 'star3',\n 'star0',\n 'star0',\n 'star0',\n 'star1',\n 'star2',\n 'star3',\n 'star2',\n 'star2',\n 'star3',\n 'star2',\n 'star4',\n 'star1',\n 'star4',\n 'star4',\n 'star4',\n 'star2',\n 'star4',\n 'star3',\n 'star1',\n 'star2',\n 'star2',\n 'star0',\n 'star1',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star2',\n 'star1',\n 'star0',\n 'star3',\n 'star4',\n 'star4',\n 'star3',\n 'star4',\n 'star4',\n 'star1',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star1',\n 'star4',\n 'star0',\n 'star3',\n 'star1',\n 'star3',\n 'star0',\n 'star2',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star0',\n 'star1',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star4',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star2',\n 'star3',\n 'star1',\n 'star2',\n 'star0',\n 'star4',\n 'star1',\n 'star0',\n 'star4',\n 'star0',\n 'star3',\n 'star4',\n 'star1',\n 'star1',\n 'star4',\n 'star3',\n 'star2',\n 'star3',\n 'star4',\n 'star2',\n 'star3',\n 'star4',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star3',\n 'star2',\n 'star2',\n 'star3',\n 'star3',\n 'star4',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star4',\n 'star0',\n 'star1',\n 'star4',\n 'star0',\n 'star4',\n 'star0',\n 'star1',\n 'star3',\n 'star4',\n 'star1',\n 'star3',\n 'star0',\n 'star2',\n 'star4',\n 'star1',\n 'star1',\n 'star3',\n 'star4',\n 'star3',\n 'star4',\n 'star4',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star3',\n 'star0',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star1',\n 'star0',\n 'star3',\n 'star3',\n 'star1',\n 'star2',\n 'star3',\n 'star1',\n 'star1',\n 'star1',\n 'star4',\n 'star2',\n 'star0',\n 'star3',\n 'star0',\n 'star3',\n 'star0',\n 'star1',\n 'star4',\n 'star4',\n 'star4',\n 'star0',\n 'star0',\n 'star4',\n 'star3',\n 'star2',\n 'star2',\n 'star4',\n 'star1',\n 'star1',\n 'star3',\n 'star0',\n 'star2',\n 'star4',\n 'star1',\n 'star3',\n 'star4',\n 'star2',\n 'star3',\n 'star2',\n 'star3',\n 'star2',\n 'star0',\n 'star3',\n 'star1',\n 'star3',\n 'star1',\n 'star3',\n 'star0',\n 'star3',\n 'star4',\n 'star1',\n 'star1',\n 'star2',\n 'star4',\n 'star3',\n 'star0',\n 'star3',\n 'star3',\n 'star0',\n 'star1',\n 'star2',\n 'star1',\n 'star3',\n 'star4',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star4',\n 'star1',\n 'star4',\n 'star1',\n 'star0',\n 'star4',\n 'star0',\n 'star2',\n 'star2',\n 'star4',\n 'star1',\n 'star4',\n 'star1',\n 'star4',\n 'star3',\n 'star3',\n 'star3',\n 'star2',\n 'star1',\n 'star1',\n 'star4',\n 'star3',\n 'star3',\n 'star0',\n 'star2',\n 'star2',\n 'star4',\n 'star1',\n 'star1',\n 'star0',\n 'star2',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star3',\n 'star2',\n 'star1',\n 'star1',\n 'star1',\n 'star3',\n 'star2',\n 'star3',\n 'star0',\n 'star4',\n 'star3',\n 'star2',\n 'star1',\n 'star4',\n 'star3',\n 'star2',\n 'star2',\n 'star4',\n 'star4',\n 'star4',\n 'star1',\n 'star2',\n 'star3',\n 'star4',\n 'star0',\n 'star4',\n 'star3',\n 'star1',\n 'star3',\n 'star2',\n 'star4',\n 'star2',\n 'star1',\n 'star3',\n 'star2',\n 'star1',\n 'star4',\n 'star3',\n 'star2',\n 'star3',\n 'star1',\n 'star1',\n 'star3',\n 'star3',\n 'star3',\n 'star3',\n 'star4',\n 'star1',\n 'star2',\n 'star1',\n 'star3',\n 'star1',\n 'star2',\n 'star0',\n 'star1',\n 'star2',\n 'star0',\n 'star2',\n 'star0',\n 'star4',\n 'star2',\n 'star2',\n 'star4',\n 'star4',\n 'star0',\n 'star3',\n 'star0',\n 'star4',\n 'star3',\n 'star4',\n 'star4',\n 'star4',\n 'star3',\n 'star4',\n 'star3',\n 'star0',\n 'star2',\n 'star2',\n 'star1',\n 'star4',\n 'star2',\n 'star1',\n 'star3',\n 'star0',\n 'star1',\n 'star3',\n ...]\n\n\n\nout_prediction('Tianhao', 'Cao', test_preds)\n\n\nfrom google.colab import files\n\nfilenames = ['best_model.pth', 'best_model_tuned.pth', 'Tianhao_Cao_PRED.txt']\n\nfor fname in filenames:\n    print(f\"downloading {fname}...\")\n    files.download(fname)\n\ndownloading best_model.pth...\n\n\n\n\n\n\n\n\ndownloading best_model_tuned.pth...\n\n\n\n\n\n\n\n\ndownloading Tianhao_Cao_PRED.txt..."
  },
  {
    "objectID": "posts/sentiment analysis.html#paper-preview",
    "href": "posts/sentiment analysis.html#paper-preview",
    "title": "Sentiment Analysis on Yelp Reviews",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "posts/credit card default analysis.html#paper-preview",
    "href": "posts/credit card default analysis.html#paper-preview",
    "title": "Credit Card Default Analysis",
    "section": "Paper Preview",
    "text": "Paper Preview\n\n\nUnable to display PDF file. Download instead."
  }
]